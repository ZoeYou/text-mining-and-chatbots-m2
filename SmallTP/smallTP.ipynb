{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text mining\n",
    "\n",
    "Date: 22/01/2021\n",
    "\n",
    "Name: Jiangnan Huang, You Zuo\n",
    "\n",
    "We will test the following analyzes:\n",
    "\n",
    "- tokenization\n",
    "- sentence segmentation\n",
    "- part-of-speech tagging\n",
    "- stemming\n",
    "- Named entity recognition\n",
    "- constituency parsing\n",
    "- dependency parsing\n",
    "\n",
    "The first four analyzes were done with the package **nltk**, while the last three were done with **StanfordCoreNLP**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T22:56:35.965220Z",
     "start_time": "2021-01-24T22:56:21.663049Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jiangnan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jiangnan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jiangnan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import itertools\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T22:56:58.489130Z",
     "start_time": "2021-01-24T22:56:57.979227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-01-24 23:56:58--  https://perso.limsi.fr/anne/tbbt.tar.gz\n",
      "Resolving perso.limsi.fr (perso.limsi.fr)... 129.175.134.198\n",
      "Connecting to perso.limsi.fr (perso.limsi.fr)|129.175.134.198|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 210807 (206K) [application/x-gzip]\n",
      "Saving to: ‘tbbt.tar.gz.3’\n",
      "\n",
      "tbbt.tar.gz.3       100%[===================>] 205,87K  --.-KB/s    in 0,03s   \n",
      "\n",
      "2021-01-24 23:56:58 (5,82 MB/s) - ‘tbbt.tar.gz.3’ saved [210807/210807]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "\n",
    "!wget https://perso.limsi.fr/anne/tbbt.tar.gz\n",
    "!tar -xzf 'tbbt.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T22:56:59.496693Z",
     "start_time": "2021-01-24T22:56:59.463847Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tbbt/s3/txt/tbbts03e01.txt',\n",
       " 'tbbt/s3/txt/tbbts03e05.txt',\n",
       " 'tbbt/s3/txt/tbbts03e04.txt',\n",
       " 'tbbt/s3/txt/tbbts03e02.txt',\n",
       " 'tbbt/s3/txt/tbbts03e03.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_ = 'tbbt/s3/txt/'\n",
    "files = [ dir_ + file for file in os.listdir(dir_)]\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T22:57:00.384324Z",
     "start_time": "2021-01-24T22:57:00.282376Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'just',\n",
       " 'want',\n",
       " 'you',\n",
       " 'both',\n",
       " 'tknow',\n",
       " ',',\n",
       " 'when',\n",
       " 'I',\n",
       " 'publish',\n",
       " 'my',\n",
       " 'findings',\n",
       " ',',\n",
       " 'I',\n",
       " 'wo',\n",
       " \"n't\",\n",
       " 'forget',\n",
       " 'your',\n",
       " 'contributions',\n",
       " '.',\n",
       " '-',\n",
       " 'Great',\n",
       " '.',\n",
       " '-',\n",
       " 'Thanks',\n",
       " '.',\n",
       " 'Of',\n",
       " 'course',\n",
       " ',',\n",
       " 'Ia']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtfile = open(files[0],'r')\n",
    "words = []\n",
    "\n",
    "for s in txtfile:\n",
    "    words.append(word_tokenize(s))\n",
    "    \n",
    "words = list(itertools.chain(*words))\n",
    "words[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T22:57:02.303322Z",
     "start_time": "2021-01-24T22:57:02.249209Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"I just want you both tknow, when I publish my findings, I won't forget your contributions.\"],\n",
       " ['- Great.'],\n",
       " ['- Thanks.'],\n",
       " [\"Of course, Ian't mention you in my Nobel acceptance speech, but when I get around to writing my memoirs, you can expect a very effusive footnote and perhaps a signed copy.\"],\n",
       " ['- We have to tell him.'],\n",
       " ['- Tell me what?'],\n",
       " ['Damn his Vulcan hearing.'],\n",
       " [\"You fellows are planning a party for me, aren't you?\"],\n",
       " ['Okay, Sheldon, sit down.'],\n",
       " ['If there\\'s going to be a them I should let you know that I don\\'t care for luau, toga or \" under the sea. \"']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtfile = open(files[0],'r')\n",
    "sentences = []\n",
    "\n",
    "for s in txtfile:\n",
    "    sentences.append(sent_tokenize(s))\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PoS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T22:57:04.015963Z",
     "start_time": "2021-01-24T22:57:03.743216Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('just', 'RB'),\n",
       " ('want', 'VBP'),\n",
       " ('you', 'PRP'),\n",
       " ('both', 'DT'),\n",
       " ('tknow', 'VBP'),\n",
       " (',', ','),\n",
       " ('when', 'WRB'),\n",
       " ('I', 'PRP'),\n",
       " ('publish', 'VBP')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = nltk.pos_tag(words)\n",
    "pos[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T22:57:06.702277Z",
     "start_time": "2021-01-24T22:57:05.197562Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'just',\n",
       " 'want',\n",
       " 'you',\n",
       " 'both',\n",
       " 'tknow',\n",
       " ',',\n",
       " 'when',\n",
       " 'I',\n",
       " 'publish',\n",
       " 'my',\n",
       " 'finding',\n",
       " ',',\n",
       " 'I',\n",
       " 'wo',\n",
       " \"n't\",\n",
       " 'forget',\n",
       " 'your',\n",
       " 'contribution',\n",
       " '.',\n",
       " '-',\n",
       " 'Great',\n",
       " '.',\n",
       " '-',\n",
       " 'Thanks',\n",
       " '.',\n",
       " 'Of',\n",
       " 'course',\n",
       " ',',\n",
       " 'Ia']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in words[:30]]\n",
    "lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T22:57:11.963683Z",
     "start_time": "2021-01-24T22:57:08.699059Z"
    }
   },
   "outputs": [],
   "source": [
    "# first install the python API and down load the StanfordCoreNLP.\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "\n",
    "nlp = StanfordCoreNLP('stanford-corenlp-full-2018-02-27')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T22:57:14.915769Z",
     "start_time": "2021-01-24T22:57:14.913075Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence = 'Paris-Saclay University is the biggest university in France, which is located in the south of Paris.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T22:57:42.175046Z",
     "start_time": "2021-01-24T22:57:15.778564Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities: [('Paris-Saclay', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('is', 'O'), ('the', 'O'), ('biggest', 'O'), ('university', 'O'), ('in', 'O'), ('France', 'COUNTRY'), (',', 'O'), ('which', 'O'), ('is', 'O'), ('located', 'O'), ('in', 'O'), ('the', 'O'), ('south', 'O'), ('of', 'O'), ('Paris', 'CITY'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "print('Named Entities:', nlp.ner(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constituency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T22:57:51.829823Z",
     "start_time": "2021-01-24T22:57:45.849684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constituency Parsing: (ROOT\n",
      "  (S\n",
      "    (NP (NNP Paris-Saclay) (NNP University))\n",
      "    (VP (VBZ is)\n",
      "      (NP\n",
      "        (NP (DT the) (JJS biggest) (NN university))\n",
      "        (PP (IN in)\n",
      "          (NP\n",
      "            (NP (NNP France))\n",
      "            (, ,)\n",
      "            (SBAR\n",
      "              (WHNP (WDT which))\n",
      "              (S\n",
      "                (VP (VBZ is)\n",
      "                  (ADJP (JJ located)\n",
      "                    (PP (IN in)\n",
      "                      (NP\n",
      "                        (NP (DT the) (NN south))\n",
      "                        (PP (IN of)\n",
      "                          (NP (NNP Paris)))))))))))))\n",
      "    (. .)))\n"
     ]
    }
   ],
   "source": [
    "print('Constituency Parsing:', nlp.parse(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-24T22:58:14.696664Z",
     "start_time": "2021-01-24T22:57:54.844093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency Parsing: [('ROOT', 0, 6), ('compound', 2, 1), ('nsubj', 6, 2), ('cop', 6, 3), ('det', 6, 4), ('amod', 6, 5), ('case', 8, 7), ('nmod', 6, 8), ('punct', 8, 9), ('nsubj', 12, 10), ('cop', 12, 11), ('acl:relcl', 8, 12), ('case', 15, 13), ('det', 15, 14), ('nmod', 12, 15), ('case', 17, 16), ('nmod', 15, 17), ('punct', 6, 18)]\n"
     ]
    }
   ],
   "source": [
    "print('Dependency Parsing:', nlp.dependency_parse(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T00:47:32.840600Z",
     "start_time": "2021-01-25T00:47:32.832102Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
